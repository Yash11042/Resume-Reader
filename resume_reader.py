# -*- coding: utf-8 -*-
"""Resume Reader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P46o4MDY4Vp6mHml0PzKwwm1ldACJQTx
"""

#!pip install langchain langchain_google_genai

# pip install -U langchain-community  
import os
import asyncio

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter

# Set your Gemini API key
os.environ['GOOGLE_API_KEY'] = 'AIzaSyBGP4PwrOSMvqSLvlrjG6DbANXqBLFpr0E'


# ðŸ”„ Ensure event loop exists in non-main threads
def ensure_event_loop():
    try:
        asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)


# âœ… Load PDF and build Vectorstore
def load_pdf(pdf_path: str, persist_directory: str = './resume_db'):
    ensure_event_loop()

    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    vectorstore.persist()
    return vectorstore


# âœ… Build Gemini LLM
def build_llm():
    return ChatGoogleGenerativeAI(
        model='gemini-1.5-flash',
        temperature=0.2,
        max_output_tokens=1024,
        google_api_key=os.environ['GOOGLE_API_KEY']
    )


# âœ… Build RAG Chain
def build_rag_chain(vectorstore):
    llm = build_llm()
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    rag_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
        memory=memory,
        return_source_documents=True,
        verbose=True
    )
    return rag_chain


# âœ… Query the chain
def query_resume(rag_chain, question: str):
    response = rag_chain({"question": question})
    answer = response["answer"]
    sources = [doc.metadata.get("source", "Unknown") for doc in response["source_documents"]]
    return answer, sources


# âœ… Main function to interact with user
if __name__ == "__main__":
    pdf_path = r'"C:\Users\gopic\OneDrive\Documents\BDA RESUME.pdf"'  # Replace with your actual file path
    vectorstore = load_pdf(pdf_path)
    rag_chain = build_rag_chain(vectorstore)

    print("\nðŸ’¬ Ask questions about the resume. Type 'exit' to quit.\n")
    while True:
        question = input("You: ")
        if question.lower() in ["exit", "quit"]:
            break
        answer, sources = query_resume(rag_chain, question)
        print("\nðŸ§  Answer:", answer)
        print("ðŸ“„ Sources:", sources)



